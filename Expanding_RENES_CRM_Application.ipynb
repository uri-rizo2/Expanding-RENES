{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOS4X1xIoEInW3HClh5qmXG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uri-rizo2/Expanding-RENES/blob/main/Expanding_RENES_CRM_Application.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3hVpIti4N30"
      },
      "source": [
        "# **Expanding RENES: CRM Application**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> **Uriel Amezcua** ||\n",
        "  **CSCI 198** || **Spring 23**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import and load files**\n",
        "\n",
        "- In this code section, I begin by installing the necessary libraries, including pandas, matplotlib, and seaborn. These libraries are essential for data manipulation, visualization, and analysis tasks.\n",
        "- Next, I import the `files` module from the `google.colab` library. This module allows me to upload files to Google Colab.\n",
        "- I then upload a CSV file containing my finished weather data. This file contains the information I need for my analysis.\n",
        "- Using the pandas library, I read the uploaded CSV file and store the data in a DataFrame called `complete_data`. This DataFrame will serve as the main data structure for my analysis.\n",
        "- To get a quick overview of the data, I display the content of the `complete_data` DataFrame. This allows me to visually inspect the data and check if it has been loaded correctly.\n",
        "- Additionally, I call the `info()` method on the `complete_data` DataFrame. This provides a concise summary of the DataFrame, including the number of rows and columns, as well as the data types of each column. This information is useful for understanding the structure of the data and identifying any potential issues or missing values."
      ],
      "metadata": {
        "id": "EMlCYeKGUr0P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nKExHh64lYT"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install pandas matplotlib seaborn\n",
        "\n",
        "# Load CSV file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import pandas as pd\n",
        "import io\n",
        "complete_data = pd.read_csv(io.BytesIO(uploaded['Finished_Weather_Data.csv']))\n",
        "\n",
        "# Display data and info\n",
        "display(complete_data)\n",
        "complete_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clean and inspect Data**\n",
        "- In this code section, I import the NumPy library and assign it the alias `np`. NumPy provides powerful numerical computing capabilities in Python.\n",
        "- Next, I set a display option in pandas to format floating-point numbers. By setting `pd.options.display.float_format` to `'{:.2f}'.format`, I ensure that floating-point values will be displayed with two decimal places.\n",
        "- Following that, I perform data cleaning operations on the `complete_data` DataFrame. First, I strip any leading or trailing white spaces from the values in the 'Ghorb0' column using the `str.strip()` method. Similarly, I apply the `str.strip()` method to the 'Ghord0' column. This step ensures that any unwanted white spaces are removed from the data.\n",
        "- Finally, I display the updated `complete_data` DataFrame. This allows me to inspect the cleaned data, ensuring that the modifications have been applied correctly. The displayed DataFrame will show the formatted floating-point numbers and the stripped values in the 'Ghorb0' and 'Ghord0' columns, respectively."
      ],
      "metadata": {
        "id": "2jnGCsRSVGf5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juHWNFiXJMLY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "##clean data from empty white space and round decimal values\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "complete_data['Ghorb0'] = complete_data['Ghorb0'].astype(str).str.strip()\n",
        "complete_data['Ghord0'] = complete_data['Ghord0'].astype(str).str.strip()\n",
        "\n",
        "\n",
        "display(complete_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmTxBmmATJgI"
      },
      "outputs": [],
      "source": [
        "complete_data[\"Ghorb0\"].replace(\"nan\", np.nan, inplace=True)  # Replace \"nan\" values with NaN in the \"Ghorb0\" column\n",
        "complete_data['Ghord0'].replace(\"nan\", np.nan, inplace=True)  # Replace \"nan\" values with NaN in the \"Ghord0\" column\n",
        "\n",
        "complete_data[\"Ghorb0\"] = pd.to_numeric(complete_data[\"Ghorb0\"])  # Convert \"Ghorb0\" column to numeric data type\n",
        "complete_data['Ghord0'] = pd.to_numeric(complete_data[\"Ghord0\"])  # Convert \"Ghord0\" column to numeric data type\n",
        "\n",
        "complete_data = complete_data.round({'Ghorb0': 3, 'Ghord0': 3})  # Round values in the \"Ghorb0\" and \"Ghord0\" columns to 3 decimal places\n",
        "\n",
        "display(complete_data)  # Display the updated complete_data DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6da7V3yJ6ZKc"
      },
      "outputs": [],
      "source": [
        "complete_data = complete_data.dropna()\n",
        "display(complete_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing and Subset Selection**\n",
        "- In this code section, I create a new DataFrame called `Neural_set` by selecting specific columns from the `complete_data` DataFrame. The selected columns are 'Ghorb0', 'Ghord0', 'Sol Rad (Ly/day)', 'Air Temp (F)', 'Rel Hum (%)', and 'cloud coverage'. This new DataFrame serves as a subset of the original data, containing only the chosen columns.\n",
        "- Next, I define a dictionary called `new_dtypes` to specify the new data types for each column in the `Neural_set` DataFrame. Each column is mapped to its corresponding data type. For example, 'Ghorb0' and 'Ghord0' are assigned the 'float16' data type, 'Rel Hum (%)' and 'cloud coverage' are assigned the 'uint8' data type, and so on.\n",
        "- Then, I use the `astype()` method to convert the columns in the `Neural_set` DataFrame to the new data types specified in the `new_dtypes` dictionary. This ensures that the columns have the desired data types for further analysis or modeling tasks.\n",
        "- Afterward, I display the updated `Neural_set` DataFrame. This allows me to verify that the data types of the columns have been successfully converted. The displayed DataFrame shows the selected columns with their updated data types.\n",
        "- Finally, there is a commented line of code to save the updated dataset to a new CSV file called 'updated_dataset.csv'.\n",
        "\n"
      ],
      "metadata": {
        "id": "VGpmzLh8WHQr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EoPt9feWpCj"
      },
      "outputs": [],
      "source": [
        "Neural_set = complete_data[['Ghorb0','Ghord0','Sol Rad (Ly/day)','Air Temp (F)','Rel Hum (%)','cloud coverage']].copy()\n",
        "\n",
        "# Define the new data types for each column\n",
        "new_dtypes = {\n",
        "    'Ghorb0': 'float16',\n",
        "    'Ghord0': 'float16',\n",
        "    'Sol Rad (Ly/day)': 'float16',\n",
        "    'Air Temp (F)': 'float16',\n",
        "    'Rel Hum (%)': 'uint8',\n",
        "    'cloud coverage': 'uint8'\n",
        "}\n",
        "\n",
        "# Convert the columns to the new data types\n",
        "Neural_set = Neural_set.astype(new_dtypes)\n",
        "\n",
        "# Save the updated dataset to a new CSV file\n",
        "#Neural_set.to_csv('updated_dataset.csv', index=False)\n",
        "\n",
        "display(Neural_set)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuration and library installation**\n",
        "1. TensorFlow Installation: I use the `pip` package manager to install the TensorFlow library, which is a powerful machine learning framework widely used for building neural networks and performing various tasks related to deep learning.\n",
        "\n",
        "2. I import specific modules from TensorFlow that I will be using in my code. These modules include `tensorflow`, `keras`, and `layers`. They provide essential functions and classes for constructing and training neural networks efficiently.\n",
        "\n",
        "3. Seaborn Installation: As part of my data visualization requirements, I install the `seaborn` library using `pip`. Seaborn is a popular data visualization library that works in conjunction with Matplotlib and offers additional functionalities and aesthetically pleasing plot styles.\n",
        "\n",
        "4. Importing Plotting Libraries: To enable visualizations and create informative plots, I import the `matplotlib.pyplot` module and the `seaborn` library.\n",
        "\n",
        "6. Importing Data Processing Libraries: In order to handle data efficiently and perform various data manipulation tasks, I import the `numpy` and `pandas` libraries.\n",
        "\n",
        "7. Configuring NumPy Printouts: To enhance the readability of NumPy arrays in my code, I configure the printout format using the `np.set_printoptions()` function. I set the precision of floating-point numbers to 3 decimal places and suppress the use of scientific notation. This ensures that the printed arrays are displayed in a clear and understandable manner.\n"
      ],
      "metadata": {
        "id": "zfm7trQmXb2o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O98cBmsymZv"
      },
      "outputs": [],
      "source": [
        "# Install the TensorFlow library and import necessary modules.\n",
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Print the version of TensorFlow installed.\n",
        "#print(tf.__version__)\n",
        "\n",
        "# Install the seaborn library for plotting.\n",
        "!pip install -q seaborn\n",
        "\n",
        "# Import necessary plotting libraries.\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Import necessary data processing libraries.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Configure NumPy printouts to be easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Splitting, Visualization, and Feature Normalization**\n",
        "\n",
        "In this section of the code, the dataset is prepared for training a machine learning model. The steps involved are as follows:\n",
        "\n",
        "1. Splitting the Data:\n",
        "   - The dataset, `Neural_set`, is split into training, validation, and test sets.\n",
        "   - The `train_dataset` is created by randomly sampling 70% of the data using `sample()` function, ensuring reproducibility with `random_state=0`.\n",
        "   - The remaining data is used to create the `test_dataset`.\n",
        "   - From the `test_dataset`, a subset is randomly sampled for the validation set using `sample()` function with `frac=0.5` and `random_state=0`.\n",
        "   - The remaining data is kept as the final `test_dataset`.\n",
        "\n",
        "2. Visualizing the Dataset:\n",
        "   - The `train_dataset` is visualized using the `sns.pairplot()` function from the seaborn library.\n",
        "   - The variables 'Ghorb0', 'Ghord0', 'Sol Rad (Ly/day)', 'Air Temp (F)', 'Rel Hum (%)', and 'cloud coverage' are selected for visualization.\n",
        "   - The diagonal elements of the plot are shown as kernel density estimation (kde) plots.\n",
        "\n",
        "3. Extracting Labels:\n",
        "   - The labels for the training, validation, and test sets are extracted from their respective datasets.\n",
        "   - The label 'Sol Rad (Ly/day)' is popped from the datasets and stored in `train_labels`, `val_labels`, and `test_labels` respectively.\n",
        "\n",
        "4. Feature Normalization:\n",
        "   - The features of the training and test sets are copied to separate variables, `train_features` and `test_features`.\n",
        "   - A `Normalization` layer is created using `tf.keras.layers.Normalization` and is adapted to the training data.\n",
        "   - The mean values of the normalizer are printed using `normalizer.mean.numpy()` to observe the normalization process.\n"
      ],
      "metadata": {
        "id": "mTOTrxLRYH7H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZo9Ezf2dtx9"
      },
      "outputs": [],
      "source": [
        "# Split the data into training, validation, and test sets\n",
        "train_dataset = Neural_set.sample(frac=0.7, random_state=0)\n",
        "test_dataset = Neural_set.drop(train_dataset.index)\n",
        "val_dataset = test_dataset.sample(frac=0.5, random_state=0)\n",
        "test_dataset = test_dataset.drop(val_dataset.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgvBEBn-dyCP"
      },
      "outputs": [],
      "source": [
        "# Visualize the dataset\n",
        "sns.pairplot(train_dataset[['Ghorb0','Ghord0','Sol Rad (Ly/day)','Air Temp (F)','Rel Hum (%)','cloud coverage']], diag_kind='kde')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi1KXPvYU5Aq"
      },
      "outputs": [],
      "source": [
        "# Extract the labels from the datasets\n",
        "train_labels = train_dataset.pop('Sol Rad (Ly/day)')\n",
        "val_labels = val_dataset.pop('Sol Rad (Ly/day)')\n",
        "test_labels = test_dataset.pop('Sol Rad (Ly/day)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VznbED6Nd_oh"
      },
      "outputs": [],
      "source": [
        "#Normalize the features\n",
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(train_features))\n",
        "\n",
        "print(normalizer.mean.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Training and Evaluation with Error Visualization**\n",
        "\n",
        "1. Model Definition and Compilation:\n",
        "   - The `build_and_compile_model` function is defined to create a sequential model using the Keras API.\n",
        "   - The model architecture consists of three dense layers with ReLU activation.\n",
        "   - The compiled model uses the Huber loss function and the Adam optimizer with a learning rate of 0.001.\n",
        "   - The defined model is stored in the `model` variable.\n",
        "\n",
        "2. Model Training:\n",
        "   - The `model.fit` function is used to train the model.\n",
        "   - The training features (`train_features`) and labels (`train_labels`) are passed as inputs.\n",
        "   - The validation split is set to 0.2, indicating that 20% of the training data will be used for validation.\n",
        "   - The `verbose` parameter is set to 0, suppressing the training progress output.\n",
        "   - The training is performed for 100 epochs.\n",
        "\n",
        "3. Model Evaluation:\n",
        "   - The trained model is evaluated on the test set using the `model.evaluate` function.\n",
        "   - The test features (`test_features`) and labels (`test_labels`) are passed as inputs.\n",
        "   - The evaluation results are stored in the `test_results` dictionary.\n",
        "\n",
        "4. Error Visualization:\n",
        "   - The prediction errors are calculated by subtracting the predicted values (`test_predictions`) from the actual labels (`test_labels`).\n",
        "   - The resulting errors are stored in the `error` variable.\n",
        "   - The `plt.hist` function is used to create a histogram of the prediction errors.\n",
        "   - The `error` values are passed as input to the function.\n",
        "   - The `bins` parameter is set to 25, indicating the number of bins in the histogram.\n",
        "   - The x-axis label is set to 'Prediction Error [total irradiance]' using `plt.xlabel`.\n",
        "   - The y-axis label is set to 'Count' using `plt.ylabel`."
      ],
      "metadata": {
        "id": "OY3hZZ2daOfA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBLSvSj6eRdX"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "def build_and_compile_model(norm):\n",
        "    model = keras.Sequential([\n",
        "        norm,\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(loss='huber_loss',\n",
        "                  optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD10TMLQeYFw"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "model = build_and_compile_model(normalizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbosGcSBedfZ"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_axLRyjeimb"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_results = {}\n",
        "test_results['model'] = model.evaluate(test_features, test_labels, verbose=0)\n",
        "\n",
        "result_table = pd.DataFrame(test_results, index=['Huber Loss [Total Radiation]']).T\n",
        "print(result_table)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cy2ailRfPU1"
      },
      "outputs": [],
      "source": [
        "# Plot the distribution of prediction errors\n",
        "error = test_predictions - test_labels\n",
        "plt.hist(error, bins=25)\n",
        "plt.xlabel('Prediction Error [total irraidance]')\n",
        "_ = plt.ylabel('Count')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Causal Inference with Gaussian Process Regression**\n",
        "1. Data Sampling:\n",
        "   - A random sampling approach is used to select 10% of the test features (`test_features`) as a representative subset.\n",
        "   - The `random.sample` function is utilized to randomly sample data points without replacement.\n",
        "   - The sampled data is stored in the `sampled_data` variable.\n",
        "\n",
        "2. Data Splitting:\n",
        "   - The sampled data is split into training and testing sets using the `train_test_split` function from scikit-learn.\n",
        "   - The `test_size` parameter is set to 0.2, indicating that 20% of the sampled data will be used for testing.\n",
        "   - The training data is stored in the `train_data` variable, and the testing data is stored in the `test_data` variable.\n",
        "\n",
        "3. Memory Usage:\n",
        "   - The size of the `sampled_data` variable is calculated using the `sys.getsizeof` function.\n",
        "   - The resulting memory size is printed to the console.\n",
        "\n",
        "4. Kernel Definition:\n",
        "   - The covariance function, or kernel, is defined for the Gaussian Process model.\n",
        "   - The Radial Basis Function (RBF) kernel is used in this case.\n",
        "\n",
        "5. Gaussian Process Model Creation:\n",
        "   - The Gaussian Process Regressor from scikit-learn is instantiated with the defined kernel.\n",
        "   - The `GaussianProcessRegressor` function is used to create the model, and the model object is stored in the `model` variable.\n",
        "\n",
        "6. Model Fitting:\n",
        "   - The Gaussian Process model is fitted to the training data using the `fit` method.\n",
        "   - The training features (`train_features`) and labels (`train_labels`) are provided as inputs to the model.\n",
        "\n",
        "7. Prediction:\n",
        "   - The trained model is used to make predictions on the test features (`test_features`).\n",
        "   - The `predict` method is called on the model, and the predicted values are stored in the `predictions` variable.\n"
      ],
      "metadata": {
        "id": "vtI_NuJxbCql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1AYXCUVACIs",
        "outputId": "8a94f9e6-5392-4379-80bf-6af471a14951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled data size: 4344 bytes\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "\n",
        "# randomly sample 10% of data\n",
        "sampled_data = random.sample(list(test_features.to_records(index=False)), int(len(test_features)*0.1))\n",
        "\n",
        "\n",
        "# split into training and testing sets\n",
        "train_data, test_data = train_test_split(sampled_data, test_size=0.2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sampled_data_size = sys.getsizeof(sampled_data)\n",
        "print(\"Sampled data size:\", sampled_data_size, \"bytes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uyGk2TRmNdbk"
      },
      "outputs": [],
      "source": [
        "# define kernel (covariance function)\n",
        "kernel = RBF()\n",
        "\n",
        "# create Gaussian process model\n",
        "model = GaussianProcessRegressor(kernel=kernel)\n",
        "\n",
        "# fit model to data\n",
        "model.fit(train_features, train_labels)\n",
        "\n",
        "# make predictions on test data\n",
        "predictions = model.predict(test_features)"
      ]
    }
  ]
}